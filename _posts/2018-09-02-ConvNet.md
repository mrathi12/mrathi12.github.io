---
series: Demystifying Deep Learning 
part: Part 8
title: Convolutional Neural Networks 
layout: default
comments: true
date:  2018-09-02 11:00:00
excerpt: Neural networks optimised for Computer Vision
image: "/assets/blog/ConvNet/convnet.png"
caption: "A convolutional neural net <em> Credit: CS31N </em>"
---

## Introduction 

In this blog post, we'll look at our first specialised neural network architecture - the **convolutional neural network**. 

## Motivation for a specialised architecture

Recall in our feedforward neural network post that the weight matrices for layer $$l$$ were stored in a $$n_l$$ x $$n_{l-1}$$ matrix. 
Therefore, the number of parameters in the first layer of the network is determined by $$n_0$$ the number of features in layer 0 (the input layer).

Images are stored in a large multidimensional array of pixels, whose dimensions are (widthxheight) for greyscale and (widthxheightx3) array for RGB images. So consider a small 256x256 RGB image - this results in 256*256*3= 196608 pixels! Each pixel is an input feature, which means that even if we only have 128 units in the first layer, the number of parameters in our weight matrix is 128*196608 = 25 million weights!

Given that most images are much more high-res than 256x256, the number of input features will increase dramatically - for a 1024x1024 RGB image the number of pixels increases by a factor of 16. 

This poses a scalability issue - our model has *far* too many weights! This requires a huge amount of memory and computational power to train. How then do we train a neural network on an image?

### Intuition 

**The key insight is as follows**:
*If we are trying to classify an object in an image, the position of that object in the image does not matter*. 

So the features we should be learning ought to be independent of position. 

This simplifying assumption is at the heart of the **convolutional neural network** (CNN).



## Convolution layer

The main difference between the convolution layer and the standard layer we used in a feedforward network is **weight sharing**. 

Each neuron in the layer looks at a square patch of the image (the *receptive field*) - looking at all of the **input channels** for that image (e.g. RGB channels for the original input image). The neurons take in input from each pixel in that patch and **share weights** regardless of the position of their patch in the image, so are *equivalent*. 

Another interpretation of the convolution layer is, rather than a bunch of equivalent neurons, that of one neuron called the **filter** / **kernel** that scans over the image, patch by patch. The size of the patch is thus called the **filter size** / **kernel size**. 


 Much like the standard neuron we have seen in a feedforward network, it weights its inputs (the pixels in that patch) and then applies an activation function to it, to output a single value for that patch. 

So the equation for the convolution layer is:






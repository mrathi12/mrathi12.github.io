---
series: Demystifying Deep Learning 
part: Part 10
title: Recurrent Neural Networks 
layout: default
comments: true
date:  2018-09-16 11:00:00
excerpt: Neural networks optimised for NLP and sequences
image: /assets/blog/RecurrentNet/rnn.png
caption: A Recurrent Neural Net <em> Image Credit - Chris Olah </em>
---

We move onto the next neural network architecture - the **recurrent neural network**, used for tasks where the input is a sequence. 

## Motivation 

As with CNNs, it is worth first considering why we have this specialised architecture. 

Our motivating example for sequence modelling is the task of **sentiment analysis** - given a movie review we want to classify it as positive or negative (*binary classification*). 

How do we determine an overall sentiment? 

For now, we can assume that each word can be mapped to an input vector (a **word embedding**) - we'll cover this mapping in more detail in a later blog post.

### Approach 1: Feed words individually

One approach is to feed each word to a feedforward neural network independently, and then average the probabilities. There is a key issue with this- *the order of the words is lost*.

Consider "good, not bad" and "bad, not good" - these phrases have two completely opposite meanings despite containing the same words.

**Order matters** - we want our neural network to use information from previous words when it classifies future words.

### Approach 2: Feed words in a batch

So to get some idea of order, we could pass in a batch of words to the network at the same time - i.e. concatenate all the word vectors. However, given that a review may be 150 words long, and each word is represented (typically) by a 300 dimensional vector, we run into a similar problem as the CNN: **too many parameters**.

Moreover, the feedforward neural net requires a fixed-size input - this involves either padding the input to ensure it is the same length, or worse, clipping short a review so that it doesn't exceed the input length. 

Both approaches have their drawbacks, so we need to use a different architecture. 

## Recurrent Network Architecture

Rather than having the huge vector for a batch of words, we want to keep track of the effect of previous words in some sort of "memory". We can do this by feeding in one word every timestep and then keep track of the output activation from the previous timestep and feed it back to the network (see above image).

Note that a given phrase could occur 50 words into the review or at the start and the effect on the sentiment would be the same. So noting this invariance across time,we can **share weights across time** - so inputs from all timesteps have the same weights.  

This solves the issue of keeping track of prior information, whilst the recurrent loop also ensures we can input **variable length sequences**. 

### Notation:

Let's define the notation we'll be using when referring to RNNs, now that we need to keep track of the timestep:

$$x^{<t>}$$ is the value of the input at timestep $$t$$, with $$y^{<t>}$$ and $$a^{<t>}$$ correspondingly the label and activation at time step $$t$$. 
The length of the $$i^{th}$$ input, $$x^{(i)<t>}$$, is denoted by $$T^{(i)}_x$$ and likewise we have $$T^{(i)}_y$$ - note these may differ. 

Based on the value of $$T^{(i)}_x$$ and $$T^{(i)}_y$$ we can split the tasks into 4 categories:
* **Many-to-Many** e.g. speech recognition, machine translation
* **Many-to-One** e.g. sentiment analysis
* **One-to-many** e.g. sentence/music generation given initial word/note
* **One-to-One** - this is the realm of standard feedforward networks



